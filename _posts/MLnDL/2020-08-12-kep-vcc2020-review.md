---
title: "[연구자로 성장하기] Audio알못에서 VCC2020참가까지 (카카오 엔터프라이즈 인턴 리뷰)"
category:
  - ML
tag:
  - Voice Conversion
  - Voice Conversion Challenge
  - VCC2020
  - KEP
  - Kakao Enterprise
  - Internship
  - 카카오엔터프라이즈
  - 인턴
sidebar_main: true
author_profile: true
use_math: true
header:
  overlay_image: https://i.imgur.com/ZBFxRDi.jpg
  overlay_filter: 0.5
published: true
---

Special Thanks to KEP / AI R&D / Audio Processing Part.

길고 길었던 카카오 엔터프라이즈 인턴 동안 업무적으로 무엇을 했는지 이야기해보고자 합니다. 정확히는 Voice Conversion이라는 task로 어떻게 Challenge를 참여하게 됬는지를 이야기해보겠습니다.

> [발표자료](https://www.slideshare.net/SubinAn1/voice-conversion-audio-vcc2020)에 있는 내용을 그대로 녹였다고 생각하시면 편할 것 같습니다. 그래서 좀 생략했으니 발표자료를 참고해주세요.
> 카엔공식테마가 아닌 제가 하나하나 만든 자료입니다.

## 나는 왜 음성처리파트로 배정되었는가?

카카오 블라인드 공채를 거쳐 카카오 브레인에서 추천을 받아 카카오 엔터프라이즈로 최종 면접 기회를 얻었고, 거기서 군필여부의 문제로 인턴으로 지원하여 합격한게 [이전까지의 이야기](/2019-10-08-kakao-blind-review.md)입니다.

면접 당시에 Kakao Enterprise는 분사하기 직전의 상태였고, 어떤 task를 주력으로 하고 있는지는 잘 몰랐습니다.
그렇기에 학부동안 딥러닝 공부에서 힘들었던 경험을 토대로 다음 마음가짐으로 면접에 임했습니다.

> 합격하면 대기업이니까 엄청 많은 데이터(자체 데이터)에 GPU 왕창 쓸 수 있는 task로 가서 하고 싶은거 해봐야겠다. 그리고 잘하는 분에게 붙어서 실력 UP해야지.

그리고 면접에서 **"만약 합격하면 어떤 분야에 가고 싶습니까?"** 라는 질문을 받게 되었고, 다음과 같은 생각을 했습니다.

- 아직 딥러닝으로 해본건 책의 예제들
  - Kaggle에서는 ML model들만 돌려본 경험
- 딥린이의 입장에서 아는 큰 갈래는 3가지
  - 텍스트 (NLP) : 근데 텍스트는 별로 관심이 없는 상황
  - 이미지/동영상 (Computer Vision)
  - 음성 (Audio)
  - etc (정형/시계열/등등)
- 관심있는 딥러닝 tasks
  - Style Transfer
  - Generation Model (especially **Music Generation**)
- 방향은 애매하지만 할 수 있다면 이런 것도 하고 싶었음
  - Dimension Reduction + Data Visualization
  - Interpretable AI (such as SHAP, CNN explainer, etc)

그런 고민들에서 저는 **음성**, **Style Transfer**, **시각화** 를 어필했습니다.
면접관 중 한 분이 음성처리파트 파트장님이셨고, 좋게 봐주신 덕분에 위의 많은 내용과들을 종합하여 **음성합성셀** 에 3개월 체험형 인턴을 시작하게 되었습니다.

![합격](https://i.imgur.com/9hiKxUN.jpg){:width="500px"}

## 합격은 했는데...

합격까지는 좋았지만 아무것도 모르는 인턴 Groot(나)는 첫 날에 상당히 부끄러웠습니다.

우선 음성합성과 Style Transfer를 하고 싶었던 만큼 Voice Conversion(목소리를 남의 목소리로 바꾸는 task)을 하게 되었습니다. 여기서 문제는 다음과 같습니다.

- Voice Conversion는 실서비스로 하기 어렵기에 회사의 main task가 되기 어렵다.
  - 어? 인턴인데 사수가 없다?!?
  - 어? 그럼 나 입사했는데 시키는게 없네?
- 그렇다면 내가 혼자 이 task를 캐리할 인재인가?
  - 이론 공부 + 아이디어성 논문만 읽은 이론충 (심지어 수학적 베이스 부족)
  - 응~ 그러니까 딥러닝 구현 경험이 없어~
  - 그리고 음성의 기본인 신호처리(DSP)도 몰라~
  - 하 내 미래 어쩌지

그러니 무슨 상황이냐? 제 인턴 업무를 담당해주신 시니어분과의 대화를 약간 각색하면...

> 음성 데이터로 뭐 해본 적 있어요? **아니 없어요**

> 딥러닝으로 음성관련 실험한건요? **없어요**

> 그럼 음성말고 구현한 논문 있어요? **아니 없어요!**

![없어요](https://i.imgur.com/blDYvBN.jpg){:width="500px"}

상당히 부끄러웠지만, 일단 관련 논문 읽고 실험할 논문을 찾아보라는 제안을 해주셔서 자연스럽게 넘어갈 수 있었습니다. (아직도 그 순간의 부끄러움이란...)

이제 그럼 제 업무는 다음과 같습니다.

- Tensorflow, Pytorch 중에서 하나의 프레임워크 연습
  - 이전에 Keras를 하긴 했는데 대세를 따르기 위해...
- 음성 데이터에 대한 이해 (DSP)
- Voice Conversion 관련 논문 탐색

이걸 3개월 기간에서 얼마나 따라갈 수 있을까 싶었습니다.

## 새로운 환경은 언제나 짜릿해

### 인턴, 원래 공부하러 들어온거지!

모든게 처음이었지만, 저는 이런 어색함과 긴장을 즐기는 편입니다. (모든 건 미화되고, 허세가 과하게 뿜뿜한편)

위에서 언급한 내용을 따라가기 위해 첫 3~4주간은 시작부터 끝까지 공부를 했습니다.
이미 딥러닝의 기본적인 이론은 알았기에 다음과 같은 내용으로 빠르게 공부했습니다. (유명한 링크만 열심히 공부했네요.)

- [Pytorch Official Tutorial](https://pytorch.org/tutorials/)
- TTS 논문 follow up : _아이디어를 얻을 수 있지 않을까?_
  - WaveNet Vocoder, Tacotron, DeepVoice, GST(Global Style Token)
  - _물론 이 내용은 많이 이해 못한 부분이 많았습니다._
- Generataion Model 이론 다지기
  - VAE류 논문 : VAE, CVAE
    - 이활석님의 [오토인코더의 모든 것](https://d2.naver.com/news/0956269)
  - GAN류 논문 : GAN, CGAN, LSGAN, CycleGAN, StyleGAN, etc
    - 최윤제님의 [1시간만에 GAN(Generative Adversarial Network) 완전 정복하기](https://tv.naver.com/v/1947034)
    - 박태성님의 [Finding connections among images using CycleGAN](http://tv.naver.com/v/2203900)
    - 유재준님의 [GAN 포스팅들](http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-1.html)
- Voice Conversion 논문 follow up
  - 최신 논문 근황은 [paperswithcode](https://paperswithcode.com/)
  - CycleGAN-VC 1&2, StyleGAN-VC 1&2

그리고 그 쯤 보고 있는 논문을 발표하고, 그 중에 실험할 논문을 찾아보라는 말씀을 하셔서 입사하고 첫 발표를 진행하게 되었습니다.

### 첫 번째 실험 : 진짜 시작이 반이더라

그리고 결론적으로 첫 실험 후보군 논문은 2개의 논문으로 추려졌고 관련으로 발표를 진행하였습니다.

- [One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization](https://arxiv.org/abs/1904.05742)
- [AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss](https://arxiv.org/abs/1905.05879)

그리고 결론적으로 첫 논문을 선택하여 약 1달 실험을 하였고, 그 결과는 다음과 같습니다.

- **[One-Shot Voice Conversion 보고서](https://subinium.github.io/Voice-Conversion/)**
  - 이걸 만들면서도 [몇 가지 이슈](/2020-02-19-github-page-and-audio-issue.md)가 있었으나...

여기서 배운 점은 다음과 같습니다.

- 딥러닝의 핵심은 **전처리와 실험** 이다. (_이래서 더 딥러닝이 싫지만ㅠㅠ_)
- 결과 확인과 새로운 실험을 항상 신경쓰자
  - preprocessing, log, configuration, etc
  - _shell은 잘하면 좋다._
- 논문은 거짓말을 많이 한다. (**_fxck cherry-picking_**)
  - 좋은 결과만 선별적으로 취사선택하는 것을 cherry picking이라고 합니다.
- 이제 딥러닝 모델 구현은 **하면 된다!!** 라는 마인드를 가지게 되었습니다.

그럼 남은 기간은 1달 남짓이었고, 실험에 대해 보충적인 부분을 해결하고 인턴 생활을 끝낼 것인가 아니면 새로운 실험을 하고 나갈 것인가의 기로에 섰습니다.

## Voice Conversion Challenge 2020, 3개월의 몰입

### 여기서 그만둘 수는 없다. 연장 가즈아!!

_"xx 모델을 돌려보았다"_ 로 인턴을 끝내고 싶지는 않았고, 관련하여 주제를 조금 탐색하던 차에 [Voice Conversion Challenge 2020(이하 VCC2020)](http://www.vc-challenge.org/)가 열리는 것을 알았습니다.

VCC2020은 이전에 VCC2016, VCC2018 두 차례의 대회가 선행된 대회로 Voice Conversion으로 여러 팀이 경쟁하는 대회입니다. 이전까지 한 내용을 적용할 수 있는 기회이자 인턴동안 그래도 결과가 남을 수 있는 시도였기에 대회를 참여하고 싶다는 욕심이 생겼습니다.

registration이 2일 정도 남아있었고, 대회를 시작하고 얼마 후에 계약기간이 끝나는 상황이 였습니다.

하지만 셀장님, 팀장님에게 인턴 연장을 부탁드렸고, 다행히 연장을 해주셔서 대회를 시작하게 되었습니다. 그리고 사수가 붙어 함께 대회를 진행하게 되었습니다.

### 대회소개 & 실험설계 : 시작은 했는데 막막하네

대회는 2가지 task로 이뤄집니다.

- **task 1 : Mono-lingual voice conversion**
  - 영어 화자 4명 -> 영어 화자 4명, 총 16 pairs
  - 여자와 남자에 대한 고려가 필요
- **task 2 : Cross-lingual voice conversion**
  - 영어 화자 4명 -> 타 언어 화자 6명, 총 24 pairs
  - task 1과 조건은 거의 유사
  - 언어는 핀란드어(Finnish), 독일어(German), 중국어(Mandarin)
  - 핀란드어/독일어/중국어 화자가 영어로 말하게 변환

그리고 결과는 2가지를 MOS(귀로 듣고 score를 매겨 평균)로 측정합니다.

- 자연스러운 음질(Naturalness)
- 유사도(Similarity)

이제 여기서 고려점은 무엇이냐? 그 당시의 고민은 다음과 같았습니다.

![Model.009](https://i.imgur.com/QQXbosD.jpg)

대충 큼직한 내용들은 다음과 같습니다.

- task 1과 task 2를 따로 진행할까?
  - 화자별로 음성이 70개 밖에 없음 (무조건 추가 데이터가 필요한 상황)
    - 아이러니하게 최종 모델에서 추가 데이터셋은 VCTK만 썼고
    - 결론적으로는 대회 측에서 제공한 980개의 음원으로 voice conversion을 했다는 신기한 story
  - 영어는 데이터셋도 어느 정도 있는 편이며 검증이 쉽다는 장점이 있음
  - 과연 task 1에 승산이 있을까?
    - VCC2018 우승 : 회사 _내부 데이터_ 쓴 중국 회사
    - ASR(automatic speech recognition) + TTS(Text-to-Speech)를 이길 수 있을까
    - ASR + TTS를 하려면 인식부터 해야하니 fail..
  - **_시간이 없으니 task1과 task2 모델을 한 번에 만들자_** 가 결론
  - **_task2에서 승산을 보자_** 가 결론
- 여기서 말하는 Similarity란?
  - 성조가 있는 영어가 중국어 화자에 가까운건지
  - 중국인 목소리의 유창한 영어가 중국어 화자에 가까운건지
  - **_성조가 있는 영어를 만들자_** 가 결론
- Too many method
  - 생각보다 방법론이 너어어어어어무 많아!!!!

이런 고민들과 이전의 방법론들을 follow-up하고 결정을 추리는데 1개월 정도 사용한 것 같습니다. 방법론을 고민하는 과정에서 저는 부가적으로 [CycleGAN-VC2](https://arxiv.org/abs/1904.04631)(_근데 이것도 cherry picking의..._)를 실험하고 있었고, GAN 특유의 깨지는 부분 등이 문제여서 일찍이 포기하게 되었습니다.

그리고 최종적으로 많은 방법론 중 2가지 논문을 베이스로 baseline model을 만들기로 했습니다.

- **Wavenet Autoencoder** : [Unsupervised speech representation learning using WaveNet autoencoders](https://arxiv.org/abs/1901.08810)
  - Wavenet 저자, VQVAE의 합리적 설명, 탑티어 논문 등의 신뢰도 상승
- **PWG Vocoder** : [Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram](https://arxiv.org/abs/1910.11480)
  - Taylor가 실험하던 모델로 위의 decoder 역할로 사용 가능
  - Wavenet보다는 빠르니 실험 상 유리

> 시작에 Taylor가 열심히 음성 기초에 대해 강의해주었는데 그게 큰 도움이 되었습니다. 이번에 카엔 블로그에 글도 쓰셨던데 많이 봐주시길 바랍니다. **[AI에게 어떻게 음성을 가르칠까?](https://tech.kakaoenterprise.com/66)**

### 중간 과정 : 1개월간 100개 이상의 실험...

대회가 1개월 남은 시점에 baseline에서 **_사람 목소리_** 가 나왔고, 그 후에는 거의 매일 `실험 -> 결과 확인 -> 개선`의 루틴을 살았습니다.

개선할 부분들을 찾으며 계속 실험했고, 전체적인 과정은 아래 사진과 같습니다.

![Model.015](https://i.imgur.com/wHUy7wx.jpg)

그림의 `/` 로 구분된 내용들은 모두 독립적으로 있으니 실험을 할 수 있는 가짓수는 기하급수적으로 늘어납니다. 매번 10~20개의 모델을 돌리고, tensorboard로 중간 결과를 듣고, 다시 돌리고, 듣고 돌리고...

그래도 GPU를 회사에서 많이 지원해주니 실험 자체에서 오는 답답함은 매우 적었습니다. 행복 그 자체.

그리고 실험 과정에서 데이터셋 탐색 및 전처리 등은 Taylor가 99% 해주셨기에 모델 구현과 실험에 많이 집중할 수 있었습니다.

> 실험을 하면 매일 아침, 점심, 저녁으로 혼자 청취평가겸 들어봤는데, 그때마다 제가 한숨을 쉬었다는 후문이...

### 최종 모델 : 딥러닝보다는 PPT에 소질이 있습니다.

결과 제출에 모델에 대한 description 및 설명을 제출해야해서 나름 `PPT깎는개발자` 인 제가 열심히 만들어서 제출했습니다. (뿌듯한 그림이니 보고가세요.)

![Model.016](https://i.imgur.com/NnD0Fnl.jpg)

VQ-VAE를 베이스로 원래 뒤에는 Wavenet이 있어야 하는데, 뒤의 부분을 PWG로 변환했습니다. 제 실험 포인트는 이미 TTS용으로 어느 정도 검증된 PWG Vocoder를 건들기보다는 VQ 부분에서 Feature Extraction이 잘 되게 tuning하는 과정이 포인트였습니다.

중간에 박사님인 Jaytee가 제안해주신대로 모델을 2개로 나누어 (Source)Audio->Feature, Feature->(Target)Audio로 했으면 더 잘될 것 같은데 시간적으로 부족해서 많이 진행하지 못했습니다.

그리고 제출하고 나서는 다음과 같은 아쉬움들이 남았습니다.
정신없이 실험한 3개월이라 알아도 다는 못했을것 같습니다.

> 멈추면 비로소 보이는 것들...

![Model.017](https://i.imgur.com/kkAW0cQ.jpg)

## Result

### 전체 성적 : 그래서 님 티어가?

열심히 한 것과 상관없이 결과가 어떻게 나왔는가를 이야기해봅시다.
7월 말에 1차 결과 8월 중순에 2차 발표되어 이제야 결과를 공유합니다.

좀 걱정되었던건 참여팀이 VCC2018에서 훨씬 늘었다는 것입니다.

VCC2018은 약 20팀 내외로 참여한 반면, VCC2020는 이메일 공지에는 약 80팀이 참여신청을 하였다 하였고, 그 후에 공식 홈페이지에서는 90+@팀이 참여했다고 하였습니다.

결론적으로는 3 baselines 포함 총 33팀이 참가했고, task 1/ task 2에 각각 31/28팀이 참여했습니다.

일본화자 206명과 영어화자 68명의 MOS 결과입니다. (영어화자는 American, British, Austrailian, Canadian, Inidian, etc 등이 있습니다.)

결과는 Naturalness / Similarity, Naturalness / Similarity / Similarity 입니다.
task2의 Similarity는 각각 English, L2(not English) language를 Reference로 삼은 값입니다.

- Japenese Result
  - task 1 : 18th / 15th
  - task 2 : 20th / 12th / **6th**
- English Result
  - task 1 : 25th / 14th
  - task 2 : 24th / 16th / **4th**

저는 Naturalness가 이렇게 안좋게 나올지 몰랐는데 아쉬웠습니다.
조금 특이한게 신기한게 대다수의 팀이 Japenese MOS가 English MOS보다 낮았는데, 저희팀만 높아졌습니다. 같은 동양인이라 듣는 귀가 비슷한가? 라는 생각도 해봅니다.

그리고 어떻게 하더라도 E2E인 저희 모델에 비해서는 ASR+TTS가 Naturalness MOS가 안나올건 알았지만 생각보다 격차가 너무 커서 놀랐습니다. 좋은 Vocoder Model을 구현해서 주셨는데 제가 활용하지 못해서 아쉽습니다.

> Audio인만큼 Energy를 체크하고, 이를 바탕으로 Normalization이라도 했으면 MOS가 더 높았을텐데 그건 아쉽습니다. 제출용 오디오를 들어본 결과 터지는 부분이 종종 발생했는데 그 이유는 Loudness라는게 제 뇌피셜.

### Similarity로 (정신)승리

처음에 목표로 삼은 Similarity 결과를 보았을 때는 만족스러운 결과를 얻었습니다. 다음은 대회측에서 제공한 결과 표 중 하나입니다. (T11, T16, T22가 baseline, T19가 저희팀)

<figure class="half">
    <img src="https://i.imgur.com/GVnrRsw.png">
    <img src="https://i.imgur.com/Pk57Nag.png">
    <figcaption>Similarity(L2 Reference) 결과. Japenese / English, T19=우리팀 </figcaption>
</figure>

또한 언어별로 Similarity 성적은 다음과 같습니다.

- Japenese Result
  - Finnish : 9th
  - German : **5th**
  - Mandarin : **1st**
- English Result
  - Finnish : 7th
  - German : **6th**
  - Mandarin : **2nd**

task 2인 cross-lingual voice conversion은 올해 처음 만들어진 task인데 나쁘지 않은 성적을 거둔 것 같아 만족스럽습니다.

거기다 중국어는 1위라해도 무방한 결과라 더더욱 뿌듯합니다.
성조를 고려하고, 수백개의 음원을 청취평가한 보람이 있네요.

<figure class="half">
    <img src="https://i.imgur.com/hG3BKyV.png">
    <img src="https://i.imgur.com/MP1r5ea.png">
    <figcaption>중국어 Similarity 결과. Japenese / English, T19=우리팀 </figcaption>
</figure>

대회에서 제공한 결과에 Similarity(L2 Reference)부문이 모두 최상위권에 있다는 점이 만족 또 만족합니다.

이렇게 Audio알못에서 Challenge까지의 결과는 끝이 났습니다.
원래는 대회가 끝나고 각 팀에서 보고서를 제출해 논문까지 이어지는 과정이 있는데, 뭔가 새로운 아이디어를 적용한 부분은 없어서 논문까지는 이어지지 못할 것 같습니다. (그런걸 고려하면 독창적인 아이디어를 고민해볼걸..? 이라는 일말의 아쉬움?)

## 마무리

대회까지 마치며 제 인턴 기간 6개월은 후회 한 점 없이 뿌듯하고 행복한 경험으로 남았습니다. 실력도 늘었고, 딥러닝 자신감도 한껏 챙겼고, 앞으로의 진로도 고민할 수 있었던 시기였습니다.

> 아이러니하게 대다수의 팀원분들은 말렸지만 대학원 진학 목표의 큰 계기 중 하나가 카엔이라는점?

대회를 준비하며 팀원분들의 도움들을 많이 받았습니다.

정말 멋진 팀에 있었다고 생각하는데, 그런 멋진 사람들과 있을 수 있고 함께 일하려면 제가 더 성장해야한다고 생각합니다.

항상 감사한 마음을 가지고, 다시 좋은 모습으로 볼 수 있게 노력해야겠습니다. :)

이렇게 제가 했던 대회에 대한 마무리가 나서 회사의 허락을 받아 글을 써봤습니다.

**이제 인턴 리뷰 끝!**

![Model.018](https://i.imgur.com/QMFDuPj.jpg)

---

> 빨리 가려면 혼자가고 멀리 가려면 함께 가라. (If you want to go fast, go alone. If you want to far go together.)
